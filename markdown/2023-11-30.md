---
Title: "2023-11-30"
# DL library
---
**(building) → training → inference**

## DL model

通过对已有预训练模型变异（模型网络结构、参数，输入数据，模型权重等），使用相同 / 不同DL library实现模型，检测模型输出的inconsistency / NaN / crash。通过人工 / 自动方法定位DL library的错误。

### training phase

- 【**[Muffin](https://www.notion.so/Muffin-96e74089e20f433ea7532773d7df9667?pvs=21)**】Muffin: testing deep learning libraries via neural architecture fuzzing. ICSE 2022. `Fuzzing, Differential Testing (cross-library), model-level, training phase`
    - 基于fuzzing生成DNN（model architecture），基于keras在不同DL library实现相同的DL models，使用differential testing检测训练阶段（forward+loss+backward）的inconsistency。
    - Bugs
        
        错误的pooling location，使用了冗余的epsilon参数来剪切输入参数，使用了错误的比较逻辑
        

### inference phase

- 【**[CRADLE](https://www.notion.so/CRADLE-0ec75cac38f646858277649232eed716?pvs=21)**】CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries. ICSE 2019. `Differential Testing (cross-library), model-level`
    - 对已有的DNN，基于Keras使用不同DL library运行预训练模型，检测并定位inconsistency。
    - Bugs
        
<img src="/2023-11-30/Untitled.png" className="img"/>        
- 【**[Audee](https://www.notion.so/Audee-635dfda37df74fbfa7930f26cba3b7e2?pvs=21)】**Audee: Automated Testing for Deep Learning Frameworks. ASE 2020. `Fuzzing, Differential Testing (cross-library), model-level, NaN`
    - 生成倾向于输出异常值（过大或过小）的DNN，以发现DL library导致的NaN错误。
    - Bugs
        - lack *neceessary check for negative values* before calculating square root
        - directly *converts the None-value to NaN*
        - *choose the pools out of the image in some cases* and further lead to *division-by-zero*, which finally triggers NaN output.
        - Full Note
            - **[BatchNormalization](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F38644&sa=D&sntz=1&usg=AOvVaw3UOCiWxaqkhGNlbfPpEMPA)** on **TensorFlow and Theano** ***Confirmed***
            
            When executing ***BatchNormalization***, TensorFlow and Theano return NaN in some cases. This is because both two frameworks lack neceessary check for negative values before calculating square root operations in ***BatchNormalization.***
            
            TensorFlow has confirmed this bug, and the relevant source code for Theano is shown in the following picture.
            
            [https://lh6.googleusercontent.com/y4ru56jhlCemZJDXLSDn76-IQF0Omh5foxBJ-xh7PnDGh6ZV49RvbOjFoGtSZ5U4cVtBvO5xBIZursIq30lSt--F2G8XTXJrvyD6QWZWGfX5Bftu=w1280](https://lh6.googleusercontent.com/y4ru56jhlCemZJDXLSDn76-IQF0Omh5foxBJ-xh7PnDGh6ZV49RvbOjFoGtSZ5U4cVtBvO5xBIZursIq30lSt--F2G8XTXJrvyD6QWZWGfX5Bftu=w1280)
            
            - **[ReLU](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F38640&sa=D&sntz=1&usg=AOvVaw1TVDk-IKQI0RLvA5HdMKDm)***(threshold=None) on TensorFlow* ***[Confirmed and Fixed](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fcommit%2F3db8df8ffafe5bcd83a12b92bc4c8287cd80237f&sa=D&sntz=1&usg=AOvVaw2Ca5ulGvbFzzhHb-_wROjv)***
            
            TensorFlow lacks necessary exception check for floating-point parameters. In case of abnormal values like None, it directly convert them to NaN, which affects the subsequent calculation.  TensorFlow has confirmed and fixed this issue as follows.
            
            [https://lh4.googleusercontent.com/kOysWpW9hsf8aXGez5Qz1YKjXXhz8kT3sf8c1Thqf1nNo_lUQhCp5d5bThhfXDTrsXU8mYXkRRqgd2WWN0QJa5Fxz658UXWZSOm7t4duDUGBuCF1=w1280](https://lh4.googleusercontent.com/kOysWpW9hsf8aXGez5Qz1YKjXXhz8kT3sf8c1Thqf1nNo_lUQhCp5d5bThhfXDTrsXU8mYXkRRqgd2WWN0QJa5Fxz658UXWZSOm7t4duDUGBuCF1=w1280)
            
            - **[LeakyReLU](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fkeras-team%2Fkeras%2Fissues%2F13787&sa=D&sntz=1&usg=AOvVaw358U9nbBP9O7gePqcgFqS8)***(alpha=None)* on TensorFlow ***Confirmed***
            
            Similar to the **ReLU** bug mentioned above. That is, TensorFlow directly converts the ***None***-value to NaN for the floating-point parameter ***alpha*** in **LeakyReLU**.
            
            - **[AvgPool2d](https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fissues%2F36977&sa=D&sntz=1&usg=AOvVaw2FsMrQ09TEY9A4zndI_0o-)** on PyTorch ***Confirmed***
            
            Under ***ceil_mode=True*** for ***AvgPool2d***, PyTorch may choose the pools out of the image in some cases and further lead to division-by-zero, which finally triggers NaN output.
            
            - **Dense/Conv/LSTM/GRU/SimpleRNN** on **Tensorflow**, **CNTK**, and **Theano**
            
            Using **exponential** activation in some APIs (e.g., **Conv2D, Dense**) can easily lead to infinity output and trigger NaN when using the infinity value in further calculations. Figure1 shows a NaN example caused by the layer ***SimpleRNN*** with the parameter ***activation=exp*** on TensorFlow.
            
            [https://lh5.googleusercontent.com/CIvkWrIYvI417njf4eEdoueK27zqU5lh7akvMIK_-cjl7N-gP8j5IHRC_kPllW_DMLHQysdTO8rQJIVRKMKZu-22gzNzH9tn4aKk45dlaFhuQFMX=w1280](https://lh5.googleusercontent.com/CIvkWrIYvI417njf4eEdoueK27zqU5lh7akvMIK_-cjl7N-gP8j5IHRC_kPllW_DMLHQysdTO8rQJIVRKMKZu-22gzNzH9tn4aKk45dlaFhuQFMX=w1280)
            
            Figure 1. NaN example on ***SimpleRNN*** with the parameter ***activation=exp*** on TensorFlow
            
            [https://lh3.googleusercontent.com/5EluQ4dkzDmfC-Bg61VHbShFyQPWnoe58t1ygjp5ihshDEF2zUMsDTz8NpCLl1dzrfdMyN7URLgq84N5Mw45ZgFxPg0M63uKiSreBX0LZDP1_V7-=w1280](https://lh3.googleusercontent.com/5EluQ4dkzDmfC-Bg61VHbShFyQPWnoe58t1ygjp5ihshDEF2zUMsDTz8NpCLl1dzrfdMyN7URLgq84N5Mw45ZgFxPg0M63uKiSreBX0LZDP1_V7-=w1280)
            
            Figure 2.   Keras implementation of ***dilation_rate*** for **SeprableConv2D/DepthwiseConv2D** on CNTK backend
            
- 【**[TensorFuzz](https://www.notion.so/TensorFuzz-1abb651cf0154956a88cc68fb333fd91?pvs=21)】**TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. ICML 2019. `Fuzzing, model-level, NaN`
    - 对已有的DNN进行fuzz，以发现在罕见输入下出现的NaN错误。
- 【[LEMON](https://www.notion.so/LEMON-1d5f1a369cec4301b30915dfd6c6dc97?pvs=21)**】**Deep learning library testing via effective model generation. ESEC/FSE 2020. `Fuzzing, Differential Testing (cross-library), model-level`
    - 对已有DL模型变异的基础上进行differential testing，检测inconsistency。检测到crash, inconsistency, NaN & Keras performance bug (caused by memory leak)。
- 【DeepHunter】
- Graph-based Fuzz Testing for Deep Learning Inference Engines. ICSE 2021.

## High-level API

- 【**Docter**】DocTer: documentation-guided fuzzing for testing deep learning API functions. ISSTA 2022. `Fuzzing, high-level API`
    - 从文档中提取API constraints进行fuzz
- 【[FuzzGPT](https://www.notion.so/FuzzGPT-4fdc1908cc8b4889b16d88c1454b3978?pvs=21)】Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT. ICSE 2024. `Fuzzing, high-level API, LLM`
    - 使用LLM（in-context: buggy code, fine-tune）合成unusual programs（corner cases API sequence）进行fuzz
- 【**[TITANFUZZ](https://www.notion.so/TITANFUZZ-76af39ce43f248a78da7cb2234eb6062?pvs=21)**】Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models. ISSTA 2023. `Fuzzing, high-level API, LLM`
    - 使用LLM生成API sequence进行fuzz
- 【[FreeFuzz](https://www.notion.so/FreeFuzz-88b2ab704f3d4a84b5e21bb7b8ce7dcd?pvs=21)】Free lunch for testing: fuzzing deep-learning libraries from open source. ICSE 2022. `Fuzzing, Differential Testing (equivalent scenario), high-level API`
    - 从open source获取code/model对high-level API进行fuzz，检测（1）CPU/GPU&CuDNN不同设置下的inconsistency（2）违反Metamorphic Relation（精度低，花销低）（3）crash。
- 【[DeepREL](https://www.notion.so/DeepREL-c54f0dcb9a3843e7b27fab9088595791?pvs=21)】Fuzzing deep-learning libraries via automated relational API inference. ESEC/FSE 2022. `Fuzzing, Differential Testing (equivalent API), high-level API`
    - 自动推理equivalent/similar APIs进行fuzz，检测relational APIs的inconsistency。
- 【**[EAGLE](https://www.notion.so/EAGLE-fe02f2873a4340bab40cd96a27dfd93f?pvs=21)】**EAGLE: Creating Equivalent Graphs to Test Deep Learning Libraries. ICSE 2022. `Differential Testing (equivalent graph), high-level API`
    - 使用等效图对单一DL库进行differential testing
    - Bugs
        - Optimization. tf.math.xdivy由于complex64除法在optimization后溢出导致overflow
            
<img src="/2023-11-30/Untitled%201.png" className="img"/>            
        - Data Structure Equivalence
            - Pytorch的torch.sspaddmm()的low-level function indices.data_ptr假设对张量的存储是行连续的，但torch.sspaddmm使用另一种存储方式。
            - Pytorch的torch.sspaddmm()在TensorFlow中没有直接对应的API，因此很难使用现有cross-library differential testing工具检测到这个问题。
            
<img src="/2023-11-30/Untitled%202.png" className="img"/>            
        - Data Format Equivalence
            
<img src="/2023-11-30/Untitled%203.png" className="img"/>            
- 【**[TENSORSCOPE](https://www.notion.so/TENSORSCOPE-6ccf42edf5ef4b9d92ad5ff2f486f245?pvs=21)】**Differential Testing of Cross Deep Learning Framework APIs: Revealing Inconsistencies and Vulnerabilities. USENIX Security 2023. `Differential Testing (cross-library), high-level API`
    - differential testing检测不同DL库的API不一致，检测等效规则下的inconsistency。
    - Bugs. inconsistency, floating point exception, reachable assertion, memory error, command injection.
        - inconsistency
            - precision bugs. 计算精度不同
            - data layout bugs. 输入数据的维度顺序不同（e.g., NHWC v.s. NCHW）
            - difference exception handling bugs. 返回的错误代码不同
        - crash
            - reachable assertion (CHECK-failure)：缺少对参数是否为标量的检查
            - OOB Read/Write. 在计算过程中存在超出分配内存的张量数组。e.g., indices和data大小不匹配
                
                ```
                data = [10, 20, 30, 40, 50]  # 数据数组，长度为5
                indices = [0, 1, 2, 5]        # 索引数组，长度为4
                offset = len(data)  # 这里offset是5
                
                for i in indices:
                    adjusted_index = i - offset  # 这可能导致负数索引
                    print(data[adjusted_index])
                ```
                
            - floating point exception (28).  (invalid operation, division by zero, overflow, underflow). 缺少corner-case检查，没有检查输入的函数是否为0。
            - command injection. 在某些用于解析输入字符串或者文件的函数中，不安全地使用eval。
                - **CVE-2022-45907：pytorch**
                    - statement不应该含有method或者函数调用
                    - [https://github.com/pytorch/pytorch/commit/767f6aa49fe20a2766b9843d01e3b7f7793df6a3](https://github.com/pytorch/pytorch/commit/767f6aa49fe20a2766b9843d01e3b7f7793df6a3)
                - **CVE-2022-45908：paddle**
                    - eval动态获取传入的窗函数名称是不安全，
                    - [https://github.com/PaddlePaddle/Paddle/commit/26c419ca386aeae3c461faf2b828d00b48e908eb](https://github.com/PaddlePaddle/Paddle/commit/26c419ca386aeae3c461faf2b828d00b48e908eb)
                    - `window_function_register.get('_' + winstr)`方法从一个预先定义好的字典中获取窗函数。这样，即使用户尝试传入一个恶意或者未定义的窗函数名称，代码也只是会抛出一个“未知窗函数类型”的错误，而不会执行任何不安全的操作。

## Low-level kernel API (Operator)

- 【**[IvySyn](https://www.notion.so/IvySyn-210372e4c9dc44bf81e4b177f2a8dabe?pvs=21)**】IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks. USENIX Security 2023. `Fuzzing, low-level API`
    - 首先对low-level kernel API进行fuzz，然后建立从high-level API到low-level API的Proof of Vulnerability，检测memory error。
    - Bugs. memory error, floating point exception, reachable assertion, command injection.
        - memory error (segfault)
            - OOB Read/Write. 在计算过程中存在超出分配内存的张量数组。e.g., indices和data大小不匹配
            - NULL-pointer dereference: 不能直接用来获得read/write primitive，但可以用来中止DL框架运行。
            - memory read/write operations at controlled address/heap address, heap overflow
        - fatal runtime error - DoS (4)
            - floating point exception. (invalid operation, division by zero, overflow, underflow). 缺少corner-case检查，没有检查输入的函数是否为0。
            - reachable assertion (CHECK-failure)
        - command injection. 在某些用于解析输入字符串或者文件的函数中，不安全地使用eval。
- 【**[NablaFuzz](https://www.notion.so/NablaFuzz-e61ec9ccf4104d2fb42a0a9039ffccf5?pvs=21)**】Fuzzing Automatic Differentiation in Deep-Learning Libraries. ICSE 2023. `Fuzzing, Differential Testing (equivalent scenario), low-level API, AD component`
    - 使用differential fuzzing检测automatic differentiation (AD)环节的API在不同的执行场景（使用不同方法实现输出/梯度计算）下的输出/梯度的inconsistency
- 【[ACETest](https://www.notion.so/ACETest-af98c7f5cc9a4bf28028423507a43e6e?pvs=21)】ACETest: Automated Constraint Extraction for Testing Deep Learning Operators. ISSTA 2023. `low-level API, DL operator`
    - 从代码中自动提取input validation constraints生成test case检测DL operators的crash
- 【[Predoo](https://www.notion.so/Predoo-a88b38a2f9f246feb3b09e30b6fdd79f?pvs=21)】Predoo: precision testing of deep learning operators. ISSTA 2021. `Fuzzing, low-level API, DL operator`
    - 基于fuzzing的operator-level精度测试方法，找到最大化operator输出的精度误差的输入条件。

# Numerical Bugs

DL library → DL program → DL model

- DL涉及大量计算相关行为
- 需要对上下文的理解（e.g., 是否存在某个算法）
- 具有一定可以提取的特征/模式

### Impact

- overflow / underflow - **NaN bugs** / **exception**
- loss of precision - inaccurate / incorrect result (**inconsistency**)

### Cause

- mathematical formula
- variable type
- algorithm

### Related Works

- 【[DeepStability](https://www.notion.so/DeepStability-5fce0e244dd744afb0151a0330f62cd2?pvs=21)】*DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning. ICSE 2022.*
- 【[DEBAR](https://www.notion.so/DEBAR-df01d9af6c6c482da615cab3ab9d2dcb?pvs=21)】*Detecting numerical bugs in neural network architectures. FSE/ESEC 2020.*
- 【[GRIST](https://www.notion.so/GRIST-581fe93d85e44054a2cc183a9565c1ab?pvs=21)】*Exposing Numerical Bugs in Deep Learning via Gradient Back-Propagation. FSE/ESEC 2021.*
- 【[RANUM](https://www.notion.so/RANUM-bc1ad5e7e78c401893ef899ea3417129?pvs=21)】*Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects.* *ICSE* *2023.*
- DeepLocalize: Fault Localization for Deep Neural Networks. ICSE 2021.

---

- Low-level kernel API除了operator以外还有别的类型吗？
- 【都能测试到哪些API？通过测试API有哪些无法/很难抵达的代码片段？】

- differential testing & differential fuzzing & fuzzing
- low-level API和operator的关系，在low-level API中除了operator还有其他别的组件吗？
- 看一下这个bug（**conv2d operator defect**）

[Predoo](https://www.notion.so/Predoo-a88b38a2f9f246feb3b09e30b6fdd79f?pvs=21)

> According to the description, conv2d should support both NHWC and NCHW format.
It is confirmed by TensorFlowers to be an API-related bug.
> 
- DL operator和传统numerical program计算实现的不同之处（[Predoo](https://www.notion.so/Predoo-a88b38a2f9f246feb3b09e30b6fdd79f?pvs=21)中总结了两者在进行testing的不同之处）